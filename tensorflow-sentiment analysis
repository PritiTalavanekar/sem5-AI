import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# 1. Create a small dataset for sentiment analysis
data = {
    'text': [
        "I love this product!",
        "This is terrible, I'm very disappointed.",
        "Amazing experience, will buy again.",
        "Worst purchase I've made, do not recommend.",
        "I am so happy with this!",
        "Very bad quality, not worth the money.",
        "This is awesome, I highly recommend it!",
        "I hate it, waste of money!",
        "It’s a great buy, I’m happy with it!",
        "I will never buy this again."
    ],
    'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative
}

# 2. Convert the dataset into a pandas DataFrame
df = pd.DataFrame(data)
print(f"Dataset:\n{df}\n")  # Print the dataset

# 3. Tokenization and padding text
max_vocab_size = 10000
max_sequence_length = 50

tokenizer = Tokenizer(num_words=max_vocab_size)
tokenizer.fit_on_texts(df['text'])

# Convert texts to sequences of integers
sequences = tokenizer.texts_to_sequences(df['text'])

# Pad sequences to ensure consistent input size
X = pad_sequences(sequences, maxlen=max_sequence_length)

# Labels
y = np.array(df['label'])

print(f"Tokenized and padded sequences:\n{X}\n")  # Print the tokenized and padded sequences

# 4. Split data into training and testing sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Training data shape: {X_train.shape}, Validation data shape: {X_valid.shape}\n")  # Print train-test split shapes

# 5. Build the CNN model
print("Building the model...\n")

model = models.Sequential([
    layers.Embedding(input_dim=max_vocab_size, output_dim=128, input_length=max_sequence_length),
    layers.Conv1D(128, 5, activation='relu'),
    layers.MaxPooling1D(pool_size=4),
    layers.Conv1D(128, 5, activation='relu'),
    layers.GlobalMaxPooling1D(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

print(f"Model architecture:\n{model.summary()}\n")  # Print model summary

# 6. Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print("Model compiled.\n")

# 7. Train the model
print("Training the model...\n")

history = model.fit(X_train, y_train, epochs=10, batch_size=2, validation_data=(X_valid, y_valid))

# 8. Evaluate the model
val_loss, val_accuracy = model.evaluate(X_valid, y_valid)
print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\n")

# 9. Predict on new data
def predict_sentiment(text):
    print(f"Predicting sentiment for text: '{text}'")
    seq = tokenizer.texts_to_sequences([text])
    padded_seq = pad_sequences(seq, maxlen=max_sequence_length)
    pred = model.predict(padded_seq)
    sentiment = "Positive" if pred[0] > 0.5 else "Negative"
    return sentiment

# Example usage
sample_text = "I absolutely love this product!"
predicted_sentiment = predict_sentiment(sample_text)
print(f"Predicted Sentiment: {predicted_sentiment}\n")

# 10. Visualize the training history
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

